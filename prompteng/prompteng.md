# Prompt & Context Engineering Patterns

실무에서 자주 쓰이는 프롬프트/컨텍스트 엔지니어링 패턴과 예시를 요약합니다.

## Generated Knowledge Prompting

- LLM에게 원하는 답변을 바로 요청하지 않고, 먼저 모델이 관련 지식을 생성하도록 요청한 다음에 이를 바탕으로 답변을 요청.

예시

- 1단계(지식 생성): "주제: 연속 최적화. 핵심 개념 5가지를 한 줄 요약으로 나열해 주세요."
- 2단계(답변 생성): "위 목록을 근거로, 대학원 신입생을 위한 500자 개요를 작성해 주세요. 각 항목에 간단한 예시를 포함하세요."

주의

- 내부 추론(Chain-of-Thought)을 노출하도록 요구하지 말고, "핵심 근거 요약" 수준으로 제한합니다.

## Retrieval Augmented Generation (RAG)

- Retrieval
- Augmented
- Generation

설명

- Retrieval: 쿼리와 유사한 문서(벡터 검색 등)를 외부 지식베이스에서 검색
- Augmented: 검색 결과를 모델 입력 컨텍스트에 주입(grounding)
- Generation: 모델이 주입 컨텍스트를 인용/근거로 답변 생성

예시 프롬프트
"아래 검색 결과만을 근거로 답변하세요. 인용은 [문서ID:줄] 형태로 표기하고, 근거가 없으면 '근거 없음'이라고 답하세요.\n[문서A:12-20]\n...요약된 본문...\n[문서B:1-5]\n...요약된 본문...\n질문: 제품 환불 정책 요약과 예외 조항은?"

팁

- Chunking(문서 분할), HyDE(가설 문서 생성), 쿼리 확장으로 검색 품질 개선
- 인용 강제, 근거 없는 응답 금지 규칙으로 환각 감소

## Cognitive Verifier Pattern

- 정확한 답변을 위해 다양한 세부 질문으로 나누어 답변을 작성, -> 종합적으로 판단해 결론을 작성

절차

1) 하위 질문 분해 2) 각 답변에 신뢰도와 근거 요약 3) 상위 결론 및 불확실성 표시

예시 프롬프트
"다음 문제를 3개의 하위 질문으로 분해하고, 각 답에 신뢰도(낮음/중간/높음)와 한줄 근거를 붙이세요. 마지막에 최종 결론과 남은 리스크를 요약하세요. 문제: 은닉층 개수가 일반화에 미치는 영향"

## metacognition

- 명시적으로 모델이 자신의 사고과정을 검토 하도록 유도
- example) 각 하위 질문에 답한 후, 그 답변에 대한 신뢰 수준을 평가하고 어떤 제약이나 불완전한 점이 있는지 추가적으로 설명해주세요. 또는, 중요한 점인데도 간과된 부분이 있는지 다시 한번 검토해 주세요.

안전 가이드

- 내부 세부 추론을 장황하게 노출하지 말고, "가정/한계/검증 필요 포인트" 요약만 요청

## Game play pattern

- LLM과 gamification 하여 더욱 흥미롭고 몰입도 있도록 하는 것
- "게임 규칙을 통해 대화 형식 or 다른 형식으로 계속 진행 가능하도록 해야함"

예시 규칙

- 역할: 당신은 면접관, 사용자는 지원자
- 라운드: 질문 1개씩, 최대 5라운드
- 평가: 각 라운드마다 강점 1개, 개선점 1개와 점수(1-5)
- 종료: 최종 총평과 다음 학습 과제 3개 제시

## Question Refinement

- 질문의 모호한 점을 알려주세요
- 질문의 특정 분야 전문가 관점으로 더 정교하게 질문을 만들어주세요

## 단계적으로 개선 요청

- 질문의 핵심 개념을 명확히 해주세요
- 이 개념을 중심으로 더 구체적인 맥락과 제약 조건을 추가 해주세요
- 최종적으로 이 질문이 특정 목적에 최적화되도록 다듬어 주세요

예시

입력: "딥러닝을 설명해줘"
출력(개선안): "대상: 비전 초급팀, 목적: CNN 도입 설득, 제약: 5분 발표, 산출: 3장 슬라이드 개요와 사례 1건"

## fact check list pattern

- llm 이 제공한 답변에 대한 사실 확인을 용이하게 하기 위한 패턴
- 질문에 대해 마지막에 신뢰도 확률로 평가
  - 출처의 신뢰성
  - 최신성
  - 학계 합의도
  - 반증 가능성 ( high, low, middle )
  - 대안 해석의 존재 여부 ( 있음/ 없음 )

체크리스트 템플릿

- 핵심 주장 목록화, 각 주장에 대해 [근거 링크/문헌], 최신성(연도), 합의도(낮음/중간/높음), 반증 사례 여부, 대안 해석 유무
- 최종 신뢰도: 낮음/중간/높음 + 한줄 근거

## meta language creation pattern

- 일종의 단축어나 코드를 사용하여 복잡한 지시나 요청을 간단하고 명확하게 모델에 전달하는 방법

## example

당신은 내 연구 보조입니다. 다음 단축어를 인식하고 설명하세요.
@ref[title] - [title]에 대한 최신 학술 연구 동향 3가지 요약
@comp[a]:[b] - [b]-[a] 의 핵심 차이점을 표로 비교
@plan[title] - [period]-[title]에 대한 [period] 연구 계획안 제안

명령어를 사용하지 않는 일반 대화에는 연구 보조원으로서 응답해 주세요.

## reflection pattern

- llm이 자신의 사고 과정을 돌아보고 성찰하고 설명하도록 요구
- 당신의 답변이 최선이라고 판단한 근거를 제시해주세요. 답변의 잠재적 한계와 유의 사항에 대해서도 설명해주세요.

구조 템플릿

- 가정: …
- 근거 요약: …
- 한계/리스크: …
- 개선 아이디어(후속 실험/데이터): …

## CO-STAR (Context, Objective, Style, Tone, Audience, Response) pattern

설명

- Context: 배경/제약/가용 리소스
- Objective: 구체적 목표/성공 기준
- Style: 서술 방식(예: 불릿/JSON/표)
- Tone: 문체(예: 간결/격식/친근)
- Audience: 독자 수준/역할
- Response: 출력 형식/스키마

예시 프롬프트
"Context: 내부 위키 요약, 2,000자 제한\nObjective: PM 온보딩용 핵심 정책 5가지 요약\nStyle: 표 + 불릿\nTone: 중립, 간결\nAudience: 경력 3년 PM\nResponse: 표(정책/설명/링크) 후 불릿 3개로 주의사항"

---

## Few-shot Prompting

- 대표 사례(입력-출력 페어) 2~5개를 제공해 형식/레벨을 학습시킴
- 예시는 간결하고 경계 사례(edge cases)를 포함

예시

Q: 이메일을 한 문장으로 요약하세요.\nA: 프로젝트 일정 지연과 예산 증액 요청.

## ReAct / Tool-Use Pattern

- 검색/계산/코드 실행 같은 외부 도구를 단계적으로 호출
- 출력에는 "도구 사용 결과 요약"만 포함(내부 추론 장문 노출 지양)

예시 프롬프트
"질문에 답하려면 웹 검색이 필요하면 검색 도구를 사용하세요. 각 단계에서 수행한 행동과 결과를 한 줄로 요약하고, 최종 답변은 5문장 내로 작성하세요."

## Structured Output (JSON/스키마)

- 파싱 가능한 형식을 강제해 자동화와 평가 용이성 확보
- JSON 스키마나 필드 정의를 명시

예시

"다음 스키마에 맞춰 JSON으로만 답하세요. 필드: title(str), bullets(list[str],<=5), risk(str in [low,medium,high])."

## Context Management(길이/품질)

- 길이 제약: 요약(Map-Reduce), 핵심 추출, 우선순위 기반 컨텍스트 구성
- 품질 제약: 최신성 체크, 출처 표기, 중복 제거, 중의성 해소
- 보안/프라이버시: 민감정보 마스킹, 비공개 데이터 격리

## Hallucination Mitigation

- 근거 인용 강제, "모르면 모름" 정책, 금지 목록(추측/허위 인용) 명시
- 숫자/날짜/코드 등 검증 가능한 요소는 계산/테스트로 확인


# 회귀 분석이란?

- 변수들 사이의 관계를 이해하고 예측하는 데 도움을 주는 통계적 방법

## 단순 선형 회귀 

- 하나의 독립 변수와 하나의 종속 변수 사이의 관계 
- x, y 와의 관계 

## 다중 선형 회귀 

- 여러 독립 변수를 사용, 종속 변수를 예측

## 로지스틱 회귀 

- 예, 아니오 같은 범주형 종속 변수를 예측

# 언제 사용하면 좋은가?

- 작은 데이터 
- 간단하고 비용이 적게드는 모델 
- 이해와 설명이 중요할 때 
- 선형 관계가 강한 데이터 


$$ Y = a + bX + \epsilon $$

X: 독립변수(independent variables)
Y: 종속변수(dependent variable), 이 변수의 값은 다른 변수에 의해 영향을 받습니다.
a: 회귀 계수, y 절편(intercept) 이라고 하며, 독립변수 X값이 0일 때 Y의 예상 값입니다.
b: 회귀 계수, 기울기(slop)이며, 독립변수 X가 한 단위 변할 때 종속변수 Y가 얼마나 변하는지를 나타냅니다.
ϵ
 : 오차(Error) 항으로, 모델이 설명하지 못하는 무작위 변동성을 나타냅니다.


# 📊 회귀 분석의 5가지 기본 가정: 완벽 가이드

회귀 분석은 강력한 도구이지만, 그 예측이 정확하고 신뢰할 수 있으려면 몇 가지 **기본적인 가정**을 충족해야 합니다. 이 가정들이 왜 중요하고, 어떻게 확인하며, 문제가 있을 때 어떻게 해결하는지 쉽고 체계적으로 알아봅시다.

<br>

## 1. 선형성 (Linearity)

### 🤔 무슨 의미인가요?
독립 변수(X)와 종속 변수(Y) 사이의 관계가 **직선** 형태라는 가정입니다. 즉, X가 한 단위 증가할 때 Y도 일정한 크기만큼 증가하거나 감소해야 합니다.

### 🌟 왜 중요한가요?
만약 관계가 곡선(비선형)인데 직선 모델(선형 회귀)로 예측하면, 모델은 데이터의 실제 패턴을 제대로 포착하지 못하고 **체계적인 오차**를 만들게 됩니다.

### 🔍 어떻게 확인하나요?
- **산점도 (Scatter Plot)**: 독립 변수(X)와 종속 변수(Y)를 축으로 하는 산점도를 그려보세요. 점들이 직선에 가깝게 분포하면 선형성이 있다고 볼 수 있습니다.
- **잔차 그림 (Residual Plot)**: 예측값(Fitted values)에 대한 잔차(Residuals)를 그려보세요. 잔차들이 0을 기준으로 무작위로 흩어져 있어야 합니다. 만약 포물선 같은 패턴이 보이면 선형성 가정이 위배된 것입니다.

### 🛠️ 어떻게 해결하나요?
- **변수 변환**: `로그(log)`, `제곱근(sqrt)`, `제곱` 등을 사용해 변수를 변환하여 관계를 선형적으로 만들 수 있습니다.
- **다항 회귀 (Polynomial Regression)**: 독립 변수의 고차항(예: $X^2$)을 모델에 추가하여 곡선 관계를 표현합니다.
- **비선형 모델 사용**: 다른 종류의 머신러닝 모델을 고려합니다.

### � 예시
- **상황**: 아이스크림 가게에서 '일일 최고 기온(X)'과 '아이스크림 판매량(Y)'의 관계를 분석합니다.
- **좋은 예 (선형성 만족)**: 기온이 1도 오를 때마다 판매량이 꾸준히 10개씩 증가합니다. 산점도를 그리면 우상향하는 직선 모양이 나옵니다.
- **나쁜 예 (선형성 위배)**: 기온이 너무 높아지면(예: 40도 이상) 사람들이 더워서 밖으로 나오지 않아 오히려 판매량이 줄어듭니다. 이 관계는 위로 볼록한 포물선 형태가 되므로 선형 회귀 모델은 이를 정확히 예측할 수 없습니다.

---

## 2. 오차의 독립성 (Independence of Errors)

### 🤔 무슨 의미인가요?
하나의 관측치에 대한 오차(실제값 - 예측값)가 다른 관측치의 오차와 **서로 관련이 없어야 한다**는 가정입니다. 특히 시간에 따라 수집된 데이터(시계열 데이터)에서 중요합니다.

### 🌟 왜 중요한가요?
오차들이 서로 관련되어 있다면(자기상관, Autocorrelation), 모델이 데이터에 숨겨진 중요한 패턴(예: 계절성, 추세)을 놓치고 있다는 신호입니다. 이는 회귀 계수의 신뢰도를 떨어뜨립니다.

### 🔍 어떻게 확인하나요?
- **더빈-왓슨 통계 (Durbin-Watson Statistic)**: 통계량이 2에 가까우면 독립적이라고 판단합니다. 0에 가까우면 양의 상관관계, 4에 가까우면 음의 상관관계가 있다고 봅니다.
- **잔차의 시계열 그래프**: 잔차를 시간 순서대로 그려보았을 때, 특정 패턴(예: 주기적인 물결 모양)이 나타나면 독립성 가정이 위배된 것입니다.

### 🛠️ 어떻게 해결하나요?
- **시계열 모델 사용**: ARIMA, SARIMA 등 자기상관을 모델링하는 기법을 사용합니다.
- **지연 변수 추가**: 이전 시점의 변수(예: 어제의 판매량)를 독립 변수로 추가합니다.

### 📈 예시
- **상황**: 주식 가격을 '거래량(X)'으로 예측하는 모델을 만들었습니다.
- **나쁜 예 (독립성 위배)**: 모델의 예측 오차를 보니, 어제 주가를 너무 높게 예측했다면(음의 오차), 오늘도 높게 예측하는(음의 오차) 경향이 나타났습니다. 이는 오차들끼리 서로 영향을 주고 있다는 의미이며, 모델이 '추세'와 같은 중요한 정보를 놓치고 있다는 증거입니다.

---

## 3. 오차의 정규성 (Normality of Errors)

### 🤔 무슨 의미인가요?
오차(Residuals)들이 평균이 0인 **정규 분포(Normal Distribution)**를 따라야 한다는 가정입니다. 즉, 오차들의 히스토그램을 그렸을 때 좌우 대칭인 종 모양이 나타나야 합니다.

### 🌟 왜 중요한가요?
이 가정이 충족되어야 회귀 계수에 대한 **통계적 가설 검정(t-test, F-test)과 신뢰구간 추정**이 정확해집니다. 즉, "이 변수가 정말 유의미한가?"라는 질문에 대한 답이 신뢰성을 갖게 됩니다.

### 🔍 어떻게 확인하나요?
- **Q-Q 플롯 (Quantile-Quantile Plot)**: 잔차가 정규 분포를 따른다면, 점들이 직선 위에 거의 정확히 놓입니다.
- **히스토그램**: 잔차의 히스토그램이 종 모양인지 확인합니다.
- **통계적 검정**: Shapiro-Wilk test, Kolmogorov-Smirnov test 등을 사용합니다.

### 🛠️ 어떻게 해결하나요?
- **데이터 크기 확인**: 표본 크기가 충분히 크면(일반적으로 30개 이상), 중심 극한 정리(Central Limit Theorem)에 의해 정규성 가정이 어느 정도 완화될 수 있습니다.
- **이상치(Outlier) 제거**: 모델 예측을 크게 벗어나는 이상치를 확인하고 제거하거나 조정합니다.
- **종속 변수 변환**: 종속 변수(Y)에 로그, 제곱근 등의 변환을 적용합니다.

### 🔔 예시
- **상황**: 학생들의 '공부 시간(X)'으로 '시험 점수(Y)'를 예측했습니다.
- **좋은 예 (정규성 만족)**: 대부분의 학생들은 예측 점수와 실제 점수 차이가 ±5점 내외(오차가 0 근처)에 몰려 있고, ±20점 이상 크게 차이 나는 경우는 매우 드뭅니다. 오차의 분포가 평균 0을 중심으로 하는 종 모양을 그립니다.
- **나쁜 예 (정규성 위배)**: 예측보다 점수가 훨씬 낮은 학생들이 유독 많다면(오차가 음수로 치우침), 오차 분포는 한쪽으로 꼬리가 긴 모양이 되어 정규성 가정을 만족하지 못합니다.

---

## 4. 오차의 등분산성 (Homoscedasticity)

### 🤔 무슨 의미인가요?
독립 변수(X)의 값과 관계없이, 오차의 **분산(퍼진 정도)이 일정해야 한다**는 가정입니다. 반대말은 이분산성(Heteroscedasticity)입니다.

### 🌟 왜 중요한가요?
분산이 일정하지 않으면(이분산성), 특정 구간의 데이터에 모델이 과도하게 영향을 받게 되어 회귀 계수의 신뢰성이 떨어집니다. 예를 들어, 특정 조건에서만 예측이 매우 불안정해집니다.

### 🔍 어떻게 확인하나요?
- **잔차 그림 (Residual Plot)**: 예측값(Fitted values)에 대한 잔차를 그렸을 때, 잔차들이 0을 기준으로 **어떤 패턴 없이** 고르게 퍼져 있어야 합니다. 만약 깔때기 모양(점점 퍼지거나 좁아지는)이 나타나면 등분산성 가정이 위배된 것입니다.
- **통계적 검정**: Breusch-Pagan test, Goldfeld-Quandt test 등을 사용합니다.

### 🛠️ 어떻게 해결하나요?
- **종속 변수 변환**: 분산이 큰 쪽의 값을 줄여주는 로그 변환 등이 효과적일 수 있습니다.
- **가중 최소 제곱(WLS) 회귀**: 오차의 분산이 작은 데이터에 더 큰 가중치를 주어 학습하는 방법을 사용합니다.

### 🎺 예시
- **상황**: 개인의 '연소득(X)'으로 '외식비 지출(Y)'을 예측합니다.
- **나쁜 예 (등분산성 위배)**:
    - 소득이 낮은 사람들은 외식비 지출이 예측과 크게 벗어나지 않습니다 (오차의 분산이 작음).
    - 소득이 높은 사람들은 외식비 지출이 매우 다양하여(아예 안 쓰거나, 매우 많이 쓰거나), 예측 오차가 매우 큽니다 (오차의 분산이 큼).
    - 이 경우 잔차 그림은 소득이 증가할수록 퍼지는 **깔때기 모양**을 보이며, 등분산성 가정을 위배합니다.

---

## 5. 다중공선성 (Multicollinearity) 부재

### 🤔 무슨 의미인가요?
다중 회귀 분석에서, **독립 변수들끼리 서로 강하게 연관되어 있지 않아야 한다**는 가정입니다.

### 🌟 왜 중요한가요?
독립 변수들 간에 상관관계가 너무 높으면, 어떤 변수가 종속 변수에 영향을 미치는지 정확히 구분하기 어렵습니다. 이로 인해 회귀 계수가 매우 불안정해지고 해석이 힘들어집니다. (예: 데이터가 조금만 바뀌어도 계수의 부호가 바뀌는 등)

### 🔍 어떻게 확인하나요?
- **상관관계 행렬 (Correlation Matrix)**: 독립 변수들 간의 상관계수를 확인합니다. 일반적으로 0.8 이상이면 다중공선성을 의심합니다.
- **분산 팽창 계수 (VIF, Variance Inflation Factor)**: VIF 값이 10 이상(엄격하게는 5 이상)이면 해당 변수에 다중공선성 문제가 있다고 판단합니다.

### 🛠️ 어떻게 해결하나요?
- **변수 제거**: 상관관계가 높은 변수 중 하나를 제거합니다.
- **변수 결합**: 상관관계 높은 변수들을 합쳐 하나의 새로운 변수로 만듭니다(예: 주성분 분석(PCA)).
- **릿지(Ridge) 회귀**: 다중공선성 문제에 덜 민감한 정규화된 회귀 모델을 사용합니다.

### 👨‍🏫 예시
- **상황**: 학생의 '키(X1)'와 '몸무게(X2)'로 '달리기 속도(Y)'를 예측합니다.
- **나쁜 예 (다중공선성 존재)**: '키'와 '몸무게'는 서로 매우 강한 양의 상관관계를 가집니다. 모델은 "키가 클수록 빠른가? 아니면 몸무게가 많이 나갈수록 빠른가?"를 명확히 구분하기 어려워합니다. 따라서 '키'의 회귀 계수가 비상식적으로 음수가 나오는 등 결과 해석이 어려워질 수 있습니다. 이럴 때는 두 변수 중 하나만 사용하거나, '체질량지수(BMI)'와 같은 새로운 변수로 결합하는 것이 좋습니다.

---

# 📈 회귀 모델 성능 평가: 완벽 정복 가이드

회귀 모델을 만들었다면, 그 모델이 "얼마나 좋은지" 객관적으로 평가할 수 있어야 합니다. 여기서는 모델의 전반적인 성능을 평가하는 지표들과 각 재료(독립 변수)의 중요성을 따져보는 방법을 전문가 수준의 깊이와 초보자의 눈높이로 설명합니다.

## 1. 회귀 모형의 적합도 평가: "우리 모델, 전체적으로 괜찮은가?"

이 지표들은 모델이 데이터 전체를 얼마나 잘 설명하는지, 즉 '숲'을 보는 지표들입니다.

### 🎯 4.1.1. 결정 계수 (R-squared, $R^2$)

#### 🤔 **초보자 눈높이 설명**
**"우리 모델이 데이터의 변동성을 몇 퍼센트나 설명하고 있는가?"** 에 대한 답입니다.
- **아파트 가격 예측 모델**을 만들었다고 상상해 보세요. 아파트 가격은 평수, 역과의 거리 등 여러 요인 때문에 변동합니다. $R^2$가 0.75라면, 우리 모델이 아파트 가격 변동의 75%를 설명하고 있다는 의미입니다. 나머지 25%는 우리 모델이 포착하지 못한 다른 요인(예: 학군, 인테리어 상태 등)이나 무작위적인 오차 때문입니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
$R^2$는 **총 변동량 중에서 모델이 설명한 변동량이 차지하는 비율**을 나타냅니다.
1.  **총 변동량 (SST, Total Sum of Squares)**: 데이터의 종속 변수(Y)가 자신의 평균값에서 얼마나 떨어져 있는지, 즉 데이터가 가진 고유의 변동성입니다. `SST = Σ(y - ȳ)²`
2.  **회귀 제곱합 (SSR, Regression Sum of Squares)**: 모델의 예측값(ŷ)이 Y의 평균값(ȳ)에서 얼마나 떨어져 있는지, 즉 모델이 설명해낸 변동성입니다. `SSR = Σ(ŷ - ȳ)²`
3.  **오차 제곱합 (SSE, Error Sum of Squares)**: 실제값(y)과 모델의 예측값(ŷ)의 차이, 즉 모델이 설명하지 못한 변동성(잔차)입니다. `SSE = Σ(y - ŷ)²`

이 세 가지의 관계는 **SST = SSR + SSE** 이며, $R^2$는 다음과 같이 계산됩니다.
$$ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} $$
$R^2$ 값이 1에 가까울수록 모델이 설명하지 못하는 오차(SSE)가 작다는 의미이므로, 모델이 데이터를 잘 설명한다고 봅니다.

#### ⚠️ **주의사항**
$R^2$는 독립 변수의 수가 늘어나면 **설명력과 상관없이 항상 증가하거나 최소한 유지**됩니다. 쓸모없는 변수를 추가해도 값이 올라가기 때문에, 맹신하면 과적합(overfitting)된 모델을 선택할 위험이 있습니다.

---

### 🛠️ 4.1.2. 조정된 결정 계수 (Adjusted R-squared)

#### 🤔 **초보자 눈높이 설명**
**"불필요한 변수에 대한 페널티를 적용한 현실적인 설명력 점수"** 입니다.
- $R^2$가 "일단 재료를 많이 넣으면 점수가 올라가는 요리 대회"라면, 조정된 $R^2$는 "불필요한 재료를 넣으면 감점하는 깐깐한 심사위원"과 같습니다. 정말로 모델 성능에 도움이 되는 변수를 추가할 때만 점수가 올라갑니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
$R^2$의 단점을 보완하기 위해 만들어졌습니다. 독립 변수의 개수(k)와 데이터의 개수(n)를 고려하여 R-제곱에 페널티를 부여합니다.
$$ \text{Adjusted } R^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - k - 1} $$
- 모델에 유의미하지 않은 변수가 추가되면, $R^2$는 소폭 증가하지만, 분모의 `(n - k - 1)`이 더 크게 감소하여 페널티가 적용되므로 조정된 $R^2$ 값은 오히려 감소할 수 있습니다. 따라서 **서로 다른 개수의 독립 변수를 가진 모델들을 비교할 때 훨씬 유용한 지표**입니다.

---

### 📏 4.1.3. 표준 오차 (Standard Error of the Regression, SER)

#### 🤔 **초보자 눈높이 설명**
**"모델의 예측이 평균적으로 얼마나 빗나가는가?"** 를 실제 데이터의 단위로 보여줍니다.
- 아파트 가격 예측 모델의 표준 오차가 '2000(만원)'이라면, 모델의 예측 가격이 평균적으로 실제 가격과 약 2000만원 정도 차이가 난다고 해석할 수 있습니다. 값이 작을수록 예측이 정교하다는 의미입니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
오차 제곱합(SSE)을 자유도로 나눈 값의 제곱근입니다. 즉, 잔차(residuals)의 표준편차와 같습니다.
$$ SER = \sqrt{\frac{SSE}{n - k - 1}} $$
$R^2$가 상대적인 설명력(%)을 보여준다면, 표준 오차는 모델의 예측 오차를 **실제 스케일**로 보여주기 때문에 모델의 실용성을 판단하는 데 매우 직관적입니다.

---

### 🔬 4.1.4. F-통계량 (F-Statistic)

#### 🤔 **초보자 눈높이 설명**
**"그래서 이 모델, 적어도 하나라도 의미 있는 변수가 있긴 한 거야?"** 라는 질문에 답합니다. 모델 전체의 유의성을 한 번에 검증합니다.
- "이 약의 레시피(모델)가 아무 효과도 없는 맹물(모든 계수=0)은 아닌가?"를 테스트하는 것과 같습니다. F-검정을 통과해야 비로소 개별 재료(변수)들의 효과를 따져볼 의미가 생깁니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
F-통계량은 **모델이 설명한 분산(MSR)을 설명하지 못한 분산(MSE)으로 나눈 값**입니다.
$$ F = \frac{\text{Explained Variance}}{\text{Unexplained Variance}} = \frac{SSR / k}{SSE / (n - k - 1)} $$
- **귀무가설 ($H_0$)**: "모든 회귀 계수는 0이다. 즉, 이 모델은 아무것도 설명하지 못한다."
- F-통계량이 크다는 것은, 모델이 설명한 분산이 설명하지 못한 분산(오차)보다 훨씬 크다는 의미입니다. 따라서 F-통계량이 충분히 크고, 이에 해당하는 p-value가 유의수준(예: 0.05)보다 작으면 귀무가설을 기각하고, "이 모델은 통계적으로 유의미하다"고 결론 내립니다.

---

### ⚖️ 4.1.5. AIC 및 BIC

#### 🤔 **초보자 눈높이 설명**
**"모델의 성능과 복잡도를 모두 고려한 종합 점수"** 입니다. (낮을수록 좋음)
- AIC와 BIC는 "설명력은 좋지만, 너무 복잡해서 이해하고 유지하기 어려운 모델"과 "조금 덜 정확해도, 단순하고 안정적인 모델" 사이에서 균형을 잡아주는 심사위원입니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
두 지표 모두 모델의 적합도(로그 우도, log-likelihood)와 모델의 복잡성(파라미터의 개수)을 함께 고려합니다.
- **AIC (Akaike Information Criterion)**: `AIC = -2 * log(L) + 2k`
- **BIC (Bayesian Information Criterion)**: `BIC = -2 * log(L) + k * log(n)`
- `log(L)`: 모델의 적합도 (클수록 좋음)
- `k`: 파라미터(독립 변수)의 개수 (복잡도에 대한 페널티)
- BIC는 데이터 개수(n)를 고려하여 AIC보다 더 강력한 페널티를 부여하는 경향이 있어, 더 단순한 모델을 선호하게 만듭니다. 여러 모델 중 **AIC 또는 BIC 값이 가장 낮은 모델**이 가장 좋은 모델로 선택됩니다.

---

## 2. 회귀 계수에 대한 T-검정: "각 재료는 과연 효과가 있는가?"

모델 전체가 유의미하다는 것을 확인했다면, 이제 각 독립 변수(재료)가 정말로 종속 변수(결과)에 영향을 미치는지 개별적으로 따져봐야 합니다.

#### 🤔 **초보자 눈높이 설명**
**"이 변수가 정말로 Y를 예측하는 데 의미 있는 역할을 하는가?"** 를 검증합니다.
- 아파트 가격 예측 모델에 '평수', '역과의 거리', '현관문 색깔'이라는 변수를 넣었다고 합시다. T-검정은 '평수'와 '역과의 거리'는 가격에 중요한 영향을 주지만, '현관문 색깔'은 아무런 영향이 없다는 것을 통계적으로 밝혀내는 역할을 합니다.

#### 🧐 **생겨난 이유와 원리 (전문가 인사이트)**
각 회귀 계수($\beta_i$)에 대해, 그 값이 통계적으로 0과 다른지를 검정합니다.
- **귀무가설 ($H_0$)**: "해당 변수의 회귀 계수 $\beta_i$는 0이다. (즉, 이 변수는 Y에 영향을 주지 않는다.)"
- **T-통계량**: `t = (추정된 계수 - 0) / (계수의 표준 오차)`
- 이 값은 **"추정된 계수 값이 0으로부터 표준 오차의 몇 배만큼 떨어져 있는가?"**를 의미합니다. t-값이 크다는 것은 우연히 0에서 멀리 떨어진 값이 나왔을 확률이 낮다는 뜻입니다.
- **p-value**: 이 t-값에 해당하는 확률 값입니다. p-value가 유의수준(예: 0.05)보다 작으면, "이 계수가 0이라는 귀무가설은 너무 희박한 확률이므로 기각한다. 따라서 이 변수는 유의미한 영향을 미친다"고 결론 내립니다.

#### 📊 **결과 해석**
- **통계적 유의성 판단**: p-value가 0.05보다 작으면, 해당 변수는 95% 신뢰수준에서 유의미하다고 말합니다.
- **변수 선택**: p-value가 높은(유의미하지 않은) 변수들은 모델에서 제거하는 것을 고려할 수 있습니다. (단, 이론적으로 중요하거나 다른 변수와의 상호작용을 고려해야 할 때는 남겨둘 수도 있습니다.)
- **신뢰 구간 추정**: "95% 신뢰구간이 [0.5, 1.5]이다"라는 것은, 우리가 같은 실험을 100번 반복하면 그중 95번은 실제 계수 값이 0.5와 1.5 사이에 있을 것이라고 확신한다는 의미입니다. 이 구간이 **0을 포함하지 않으면** 해당 계수는 통계적으로 유의미하다고 해석할 수 있습니다.
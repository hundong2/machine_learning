{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37613890",
   "metadata": {},
   "source": [
    "# One-Hot Encoding (원-핫 인코딩) — 기초부터 실무까지\n",
    "\n",
    "이 노트북은 명목형(범주형) 변수를 수치형으로 변환하는 대표 기법인 'One-Hot Encoding'을 이론과 코드 예제로 \n",
    "직접 실행하며 배우도록 구성했습니다. pandas, scikit-learn 예제를 포함합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7a458",
   "metadata": {},
   "source": [
    "## 학습 목표 (Checklist)\n",
    "- One-Hot Encoding의 개념과 목적 이해\n",
    "- pandas의 `get_dummies` 사용법 숙지\n",
    "- scikit-learn의 `OneHotEncoder` 사용법(희소행렬, handle_unknown, drop 등) 이해\n",
    "- ColumnTransformer와 Pipeline에서의 적용 방법 실습\n",
    "- 다중공선성, 고차원 범주, unseen category 처리 같은 실무 이슈와 대처법 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177bec6",
   "metadata": {},
   "source": [
    "## 핵심 개념 요약\n",
    "- 범주형 변수를 수치형으로 바꿔 모델이 다룰 수 있게 한다.\n",
    "- 각 범주를 이진 변수(0/1)로 변환: 예) 색상 {red, blue, green} -> color_red, color_blue, color_green.\n",
    "- 장점: 의미 보존(순서 정보가 없는 범주에 적절), 많은 모델(특히 선형 모델)에 유용.\n",
    "- 단점: 변수 수 폭발(카디널리티가 높을 때), 다중공선성(모든 더미를 사용하면 완전공선성 발생), 희소 표현과 메모리 문제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 로드\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abffc3f",
   "metadata": {},
   "source": [
    "## 간단한 데이터셋 생성 — 명목형 컬럼 예시\n",
    "실습용으로 작은 데이터프레임을 만들어 `get_dummies`와 `OneHotEncoder`를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red'],\n",
    "    'size': ['S', 'M', 'L', 'M', 'S'],\n",
    "    'price': [10, 15, 20, 15, 12]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf24b8",
   "metadata": {},
   "source": [
    "## pandas: get_dummies 사용법\n",
    "- 가장 간단한 방법. DataFrame을 그대로 받아 새로운 더미 컬럼을 만들어 반환한다.\n",
    "- `drop_first=True`로 한 컬럼을 제거해 회귀 계열의 다중공선성을 완화할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 사용\n",
    "pd.get_dummies(df, columns=['color', 'size'])\n",
    "\n",
    "# drop_first=True 예시(참고)\n",
    "pd.get_dummies(df, columns=['color', 'size'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c36a0",
   "metadata": {},
   "source": [
    "## scikit-learn: OneHotEncoder\n",
    "- sklearn의 인코더는 파이프라인에 쉽게 결합된다.\n",
    "- 주요 옵션: `sparse`(희소행렬 출력 여부), `handle_unknown`(훈련에 없는 범주 처리), `drop`(첫 카테고리 제거) 등."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0bf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')  # 작은 예제이므로 dense로 설정\n",
    "X = df[['color', 'size']]\n",
    "ohe.fit(X)\n",
    "ohe.transform(X)\n",
    "\n",
    "# feature 이름 확인(0.24+ 에서 get_feature_names_out 사용)\n",
    "try:\n",
    "    print('feature names:', ohe.get_feature_names_out(['color','size']))\n",
    "except Exception:\n",
    "    print('sklearn version may not support get_feature_names_out; use older get_feature_names if needed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5766c",
   "metadata": {},
   "source": [
    "## ColumnTransformer와 Pipeline에서 범주형 처리하기\n",
    "실무에서는 수치형/범주형 전처리를 분리하여 ColumnTransformer로 묶어 파이프라인에 넣는 것이 추천됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 파이프라인 예시: OneHotEncoding + LogisticRegression(데모 목적)\n",
    "# 여기서는 price>12 를 타깃으로 간단한 분류 문제를 만들어 테스트합니다.\n",
    "df2 = df.copy()\n",
    "df2['target'] = (df2['price'] > 12).astype(int)\n",
    "X = df2[['color','size']]\n",
    "y = df2['target']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), ['color','size'])],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "clf = Pipeline([('pre', preprocessor), ('clf', LogisticRegression())])\n",
    "clf.fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "print('accuracy (train):', accuracy_score(y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c007919",
   "metadata": {},
   "source": [
    "## drop='first' 와 다중공선성\n",
    "- 모든 더미 컬럼을 사용하면 sum(더미들)=1 관계 때문에 완전공선성(완벽한 상관)이 생긴다.\n",
    "- 선형/로지스틱 모델에서 이를 피하려면 `drop='first'` 또는 `drop_first=True`를 사용해 하나의 카테고리를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop='first' 비교 (OneHotEncoder의 drop 사용 예)\n",
    "ohe_all = OneHotEncoder(sparse=False, handle_unknown='ignore', drop=None)\n",
    "ohe_drop = OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')\n",
    "X = df[['color','size']]\n",
    "print('all dims:', ohe_all.fit_transform(X).shape)\n",
    "print('drop first dims:', ohe_drop.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd7611",
   "metadata": {},
   "source": [
    "## 실제 운영/실무 고려사항 (요약)\n",
    "- 카디널리티가 매우 높은 범주(예: 수십만 user id): one-hot은 불가능에 가깝다 -> hashing, target encoding 사용 검토.\n",
    "- 훈련 시 없는 새로운 범주가 운영에서 들어올 수 있음 -> `handle_unknown='ignore'` 권장 또는 새 카테고리 묶음(예: 'OTHER') 처리가 필요.\n",
    "- 희소행렬을 사용하면 메모리 절약 가능(sparse=True). 일부 모델이 희소 행렬을 직접 받아들임.\n",
    "- 선형 모델에서는 drop 첫 카테고리를 통해 다중공선성 완화. 트리 기반 모델은 원-핫을 써도 큰 문제가 되지 않음(트리 자체가 범주를 분할)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbcd437",
   "metadata": {},
   "source": [
    "## 고카디널리티 대안 간단 시연: FeatureHasher(해싱 트릭)\n",
    "- FeatureHasher는 텍스트나 범주형의 빠른 해싱 기반 인코딩을 제공.\n",
    "- 충돌(hash collision)이 발생할 수 있으므로 결과를 해석하기 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해싱 예시 (범주를 문자열로 변환하여 FeatureHasher 적용)\n",
    "fh = FeatureHasher(n_features=8, input_type='string')\n",
    "cats = df['color'].astype(str).tolist()\n",
    "X_hash = fh.transform(cats)  # 희소행렬 반환\n",
    "print('hashed shape:', X_hash.shape)\n",
    "print(X_hash.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e1164",
   "metadata": {},
   "source": [
    "## 실전 팁(마무리)\n",
    "- 빠른 실험: `pd.get_dummies`로 시작해 모델 성능과 리소스 사용량을 확인.\n",
    "- 프로덕션: ColumnTransformer + Pipeline으로 재현 가능 파이프라인 구성.\n",
    "- unseen 카테고리 대비: `handle_unknown='ignore'` 또는 validation/feature engineering 단계에서 'OTHER'로 묶기.\n",
    "- 고카디널리티: 해싱 또는 타깃 인코딩(통계적 방법) 고려.\n",
    "\n",
    "---\n",
    "만약 셀을 바로 실행해 결과 확인을 원하면 알려주세요. 필요하면 각 예제에 대한 더 깊은 해설(수학적 배경, 확률적 처리 방법, 성능 비교 표)도 추가하겠습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
